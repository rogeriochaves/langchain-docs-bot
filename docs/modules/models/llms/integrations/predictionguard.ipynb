{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RqWPav7AtKL"
      },
      "outputs": [],
      "source": [
        "! pip install predictionguard langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import predictionguard as pg\n",
        "from langchain.llms import PredictionGuard\n",
        "from langchain import PromptTemplate, LLMChain"
      ],
      "metadata": {
        "id": "2xe8JEUwA7_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic LLM usage\n",
        "\n"
      ],
      "metadata": {
        "id": "mesCTyhnJkNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional, add your OpenAI API Key. This is optional, as Prediction Guard allows\n",
        "# you to access all the latest open access models (see https://docs.predictionguard.com)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<your OpenAI api key>\"\n",
        "\n",
        "# Your Prediction Guard API key. Get one at predictionguard.com\n",
        "os.environ[\"PREDICTIONGUARD_TOKEN\"] = \"<your Prediction Guard access token>\""
      ],
      "metadata": {
        "id": "kp_Ymnx1SnDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pgllm = PredictionGuard(model=\"OpenAI-text-davinci-003\")"
      ],
      "metadata": {
        "id": "Ua7Mw1N4HcER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pgllm(\"Tell me a joke\")"
      ],
      "metadata": {
        "id": "Qo2p5flLHxrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control the output structure/ type of LLMs"
      ],
      "metadata": {
        "id": "EyBYaP_xTMXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Respond to the following query based on the context.\n",
        "\n",
        "Context: EVERY comment, DM + email suggestion has led us to this EXCITING announcement! ðŸŽ‰ We have officially added TWO new candle subscription box options! ðŸ“¦\n",
        "Exclusive Candle Box - $80 \n",
        "Monthly Candle Box - $45 (NEW!)\n",
        "Scent of The Month Box - $28 (NEW!)\n",
        "Head to stories to get ALLL the deets on each box! ðŸ‘† BONUS: Save 50% on your first box with code 50OFF! ðŸŽ‰\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Result: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"query\"])"
      ],
      "metadata": {
        "id": "55uxzhQSTPqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Without \"guarding\" or controlling the output of the LLM.\n",
        "pgllm(prompt.format(query=\"What kind of post is this?\"))"
      ],
      "metadata": {
        "id": "yersskWbTaxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With \"guarding\" or controlling the output of the LLM. See the \n",
        "# Prediction Guard docs (https://docs.predictionguard.com) to learn how to \n",
        "# control the output with integer, float, boolean, JSON, and other types and\n",
        "# structures.\n",
        "pgllm = PredictionGuard(model=\"OpenAI-text-davinci-003\", \n",
        "                        output={\n",
        "                                \"type\": \"categorical\",\n",
        "                                \"categories\": [\n",
        "                                    \"product announcement\", \n",
        "                                    \"apology\", \n",
        "                                    \"relational\"\n",
        "                                    ]\n",
        "                                })\n",
        "pgllm(prompt.format(query=\"What kind of post is this?\"))"
      ],
      "metadata": {
        "id": "PzxSbYwqTm2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chaining"
      ],
      "metadata": {
        "id": "v3MzIUItJ8kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pgllm = PredictionGuard(model=\"OpenAI-text-davinci-003\")"
      ],
      "metadata": {
        "id": "pPegEZExILrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)\n",
        "\n",
        "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
        "\n",
        "llm_chain.predict(question=question)"
      ],
      "metadata": {
        "id": "suxw62y-J-bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Write a {adjective} poem about {subject}.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)\n",
        "\n",
        "llm_chain.predict(adjective=\"sad\", subject=\"ducks\")"
      ],
      "metadata": {
        "id": "l2bc26KHKr7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I--eSa2PLGqq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}